                                        DMDS SLIPS        
data = pd.read_csv('diabetes.csv')
print("Dataset Preview:")
                      	       	           	SLIP 1
Q1. Write a R program to add, multiply and divide two vectors of integertype.(Vector length should be minimum 4). 10m
Ans:- # Define two integer vectors of length 4
vector1 <- c(10, 20, 30, 40)
vector2 <- c(2, 4, 6, 8)
add_result <- vector1 + vector2
multiply_result <- vector1 * vector2
divide_result <- vector1 / vector2
cat("Addition of two vectors:\n", add_result, "\n")
cat("Multiplication of two vectors:\n", multiply_result, "\n")
cat("Division of two vectors:\n", divide_result, "\n")
 
Q.2 Consider the student data set. It can be downloaded from: https://drive.google.com/open?id=1oakZCv7g3mlmCSdv9J8kdSaqO 5_6dIOw . Write a programme in python to apply simple linear regression and find out mean absolute error, mean squared error and root mean squared error.
Ans:-
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
 
# Load the dataset
import pandas as pd
data = pd.read_csv("student_scores.csv")
 
 
# Split the data into features (X) and target (y)
X = data[['Hours']]
y = data['Scores']
 
# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
 
# Create a linear regression model
model = LinearRegression()
 
# Train the model on the training data
model.fit(X_train, y_train)
 
# Predict the test set results
y_pred = model.predict(X_test)
 
# Calculate the error metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
 
# Print the results
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
 
                                                                    
 
 
 
SLIP 2
Q.1 Write an R program to calculate the multiplication table using a function.
Ans:- # Function to print the multiplication table of a given number
# Define a function to calculate multiplication table
multiplication_table <- function(num, up_to = 10) {
  cat("Multiplication Table for", num, "\n")
  for (i in 1:up_to) {
    result <- num * i
    cat(num, "x", i, "=", result, "\n")
  }
}
multiplication_table(5) 
Q.2 Write a python program to implement k-means algorithms on asynthetic dataset.
Ans:-import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import make_blobs
n_samples = 300
n_features = 2
n_clusters = 3	
random_state = 42
 
X, y = make_blobs(n_samples=n_samples, centers=n_clusters, n_features=n_features, random_state=random_state)
data = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])
plt.figure(figsize=(10, 6))
plt.scatter(data['Feature_1'], data['Feature_2'], s=30)
plt.title('Synthetic Dataset')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid()
plt.show()
def k_means(X, n_clusters, n_iterations=100):
    centroids = X.sample(n_clusters).to_numpy()
	
    for _ in range(n_iterations):
        distances = np.linalg.norm(X.to_numpy()[:, np.newaxis] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        new_centroids = np.array([X.to_numpy()[labels == k].mean(axis=0) for k in range(n_clusters)])
        if np.all(centroids == new_centroids):
            break
        centroids = new_centroids
    return labels, centroids
labels, centroids = k_means(data, n_clusters)
plt.figure(figsize=(10, 6))
plt.scatter(data['Feature_1'], data['Feature_2'], c=labels, s=30, cmap='viridis', marker='o', edgecolor='k')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.75, marker='X') 
plt.title('K-Means Clustering Results')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid()
plt.show() 

                                                     SLIP 3
Q.1 Write a R program to reverse a number and also calculate the sum of digits of that number.
Ans:- rev_sum<-function(num){
  rev<-0
  sum<-0
  while(num>0){
   rem<-num%%10
   sum<-sum+rem
   rev<-rev*10+rem
   num<-floor(num/10)
  }
  cat("Reverse of Given Number is : ",rev)
  cat("\n Sum of Given Number is : ",sum)
}
rev_sum(123)Q.2 Consider the following observations/data. And apply simple linear regression and find out estimated coefficients b0 and b1.( use numpypackage) 
x=[0,1,2,3,4,5,6,7,8,9,11,13] 
y = ([1, 3, 2, 5, 7, 8, 8, 9, 10, 12,16, 18]
Ans:- import numpy as np
x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13])
y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12, 16, 18])
A = np.vstack([x, np.ones(len(x))]).T
b, a = np.linalg.lstsq(A, y, rcond=None)[0]
print(f'Estimated coefficients:\n b = {b:.2f} \n a = {a:.2f}')
                                               	
SLIP 4
Q.1 Write a R program to calculate the sum of two matrices of given size
Ans:- # Function to add two matrices
add_matrices <- function(matrix1, matrix2) {
  if (all(dim(matrix1) == dim(matrix2))) {
    return(matrix1 + matrix2)
  } else {
    stop("Matrices must have the same dimensions!")
  }
}
matrix1 <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3)
matrix2 <- matrix(c(9, 8, 7, 6, 5, 4, 3, 2, 1), nrow = 3, ncol = 3)
sum_matrix <- add_matrices(matrix1, matrix2)
cat("Matrix 1:\n")
print(matrix1)
cat("\nMatrix 2:\n")
print(matrix2)
cat("\nSum of the matrices:\n")
print(sum_matrix)
Q.2 Consider following dataset weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny','Rainy','Sunn y','Overcast','Overcast','Rainy'] temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild'] play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']. Use Naïve Bayes algorithm to predict [0: Overcast, 2: Mild]tuple belongs to which class whether to play the sports or not.
Ans:- import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder
weather = ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy']
temp = ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild']
play = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
data = pd.DataFrame({
    'Weather': weather,
    'Temperature': temp,
    'Play': play
})
label_encoders = {}
for column in ['Weather', 'Temperature', 'Play']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le
X = data[['Weather', 'Temperature']]
y = data['Play']
nb = GaussianNB()
nb.fit(X, y)
feature_tuple = [0, 2]  # Corresponds to [Overcast, Mild]
prediction = nb.predict([feature_tuple])
predicted_class = label_encoders['Play'].inverse_transform(prediction)
print(f"The predicted class for the feature tuple {feature_tuple} is: {predicted_class[0]}")
	
                                       	  
                                               	SLIP 5
Q.1. Write a R program to concatenate two given factors.
Ans:- factor1 <- factor(c("apple", "banana", "cherry"))
factor2 <- factor(c("orange", "banana", "grape"))
concatenated_factors <- factor(c(as.character(factor1), as.character(factor2)))
cat("Factor 1:\n")
print(factor1)
cat("\nFactor 2:\n")
print(factor2)
cat("\nConcatenated Factors:\n")
print(concatenated_factors)
Q.2Write a Python program build Decision Tree Classifier using Scikit- learn package for diabetes data set (download database from https://www.kaggle.com/uciml/pimaindians-diabetes-database)
Ans:- import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn import tree
import matplotlib.pyplot as plt
data = pd.read_csv("diabetes.csv" )
print(data.head())
X = data.drop(columns='Outcome')  # Features
y = data['Outcome']  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
plt.figure(figsize=(20, 10))
tree.plot_tree(clf, feature_names=X.columns, class_names=['Non-Diabetic', 'Diabetic'], filled=True)
plt.show()
 
                                                          SLIP 6
Q.1 Write a R program  to create a data frame using two given vectors and display the duplicate elements.
Ans:- # Define two vectors
vector1 <- c(1, 2, 3, 4, 5, 3, 2)
vector2 <- c("apple", "banana", "cherry", "apple", "grape", "banana", "apple")
data_frame <- data.frame(Number = vector1, Fruit = vector2)
cat("Data Frame:\n")
print(data_frame)
duplicate_numbers <- vector1[duplicated(vector1)]
duplicate_fruits <- vector2[duplicated(vector2)]
cat("\nDuplicate elements in the 'Number' column:\n")
print(unique(duplicate_numbers))
cat("\nDuplicate elements in the 'Fruit' column:\n")
print(unique(duplicate_fruits))
 
Q.2 Write a python program to implement hierarchical Agglomerative clustering algorithm. (Download Customer.csv dataset from github.com).
Ans:- import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
url = 'wholesale-customer.csv'
df = pd.read_csv(url)
data = df.drop(['Channel', 'Region'], axis=1)
Z = linkage(data, method='ward')
plt.figure(figsize=(12, 8))
dendrogram(Z, truncate_mode='level', p=3)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Customers')
plt.ylabel('Distance')
plt.grid()
plt.show()
n_clusters = 3
clusters = fcluster(Z, n_clusters, criterion='maxclust')
df['Cluster'] = clusters
plt.figure(figsize=(10, 6))
plt.scatter(df['Fresh'], df['Milk'], c=df['Cluster'], cmap='viridis', s=100)
plt.title('Hierarchical Clustering of Wholesale Customers')
plt.xlabel('Fresh Products')
plt.ylabel('Milk Products')
plt.grid()
plt.colorbar(label='Cluster')
plt.show()
 
                                                             SLIP 7
Q.1 Write a R program to create a sequence of numbers from 20 to 50 and find the mean of numbers from 20 to 60 and sum of numbers from 51 to 91.
Ans:-sequence_20_to_50 <- seq(20, 50)
mean_20_to_60 <- mean(20:60)
sum_51_to_91 <- sum(51:91)
cat("Sequence from 20 to 50:\n")
print(sequence_20_to_50)
cat("\nMean of numbers from 20 to 60:\n", mean_20_to_60, "\n")
cat("\nSum of numbers from 51 to 91:\n", sum_51_to_91, "\n")
 
Q.2 Consider the following observations/data. And apply simple linear regression and find out estimated coefficients b1 and b1 Also analyze the performance of the model (Use sklearn package)
 x = np.array([1,2,3,4,5,6,7,8]) y = np.array([7,14,15,18,19,21,26,23])
Ans:- 
import numpy as np
from sklearn.linear_model import LinearRegression

# Data
x = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(-1, 1)  # Reshape to a 2D array for LinearRegression
y = np.array([7, 14, 15, 18, 19, 21, 26, 23])

# Create and fit the model
model = LinearRegression().fit(x, y)

# Get the estimated coefficients
b = model.coef_[0]  # Slope (b)
a = model.intercept_  # Intercept (a)

# Print the coefficients
print(f'Estimated coefficients:\n b = {b:.2f} \n a = {a:.2f}')

# Make predictions
y_pred = model.predict(x)

# Calculate Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
mse = np.mean((y - y_pred) ** 2)
rmse = np.sqrt(mse)

# Print MSE and RMSE
print(f'Mean Squared Error: {mse:.2f}')
print(f'Root Mean Squared Error: {rmse:.2f}')

 
 
                                                        SLIP 8
Q.1 . Write a R program to get the first 10 Fibonacci numbers.
Ans fibo<-function(num){
  x<-0
  y<-1
  print("Fibonnaci Series  Are :")
  print(x)
  print(y)
  for(i in (3:num)){
   z<-x+y
   print(z)
   x<-y
   y<-z
 }
}
fibo(10)
Q.2 Write a python program to implement k-means algorithm to build prediction model (Use Credit Card Dataset CC GENERAL.csv Download from kaggle.com)
Ans:- import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd
df = pd.read_csv('CC GENERAL.csv')
print(df.head())
df.dropna(inplace=True)
features = df.drop(columns=['CUST_ID']).values  # Using .values to get numpy array
def kmeans(X, k, max_iters=100):
    centroids = X[nm.random.choice(X.shape[0], k, replace=False)]
     for _ in range(max_iters):
        distances = nm.linalg.norm(X[:, nm.newaxis] - centroids, axis=2)
        labels = nm.argmin(distances, axis=1)
            	new_centroids = nm.array([X[labels == i].mean(axis=0) for i in range(k)])
            	if nm.all(centroids == new_centroids):
            break
          centroids = new_centroids
          return labels, centroids
k = 4  # You can adjust this based on your analysis
labels, centroids = kmeans(features, k)
df['Cluster'] = labels
print(df.head())
mtp.scatter(features[:, 0], features[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k')
mtp.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.75, marker='X')  # Plot centroids
mtp.title('K-Means Clustering of Credit Card Data')
mtp.xlabel('Feature 1')
mtp.ylabel('Feature 2')
mtp.show()
 
                                                        SLIP 9
Q.1   Write an R program to create a Data frames which contain details of 5 employees and display summary of the data.
Ans:- # Step 1: Create the Data Frame with details of 5 employees
employee_data <- data.frame(
  Employee_ID = c(101, 102, 103, 104, 105),
  Name = c("John", "Sara", "Mike", "Anne", "David"),
  Age = c(28, 34, 25, 30, 40),
  Department = c("HR", "Finance", "IT", "Marketing", "Finance"),
  Salary = c(50000, 60000, 45000, 55000, 70000)
)

# Step 2: Print the data frame
print(employee_data)

# Step 3: Display the summary of the data
summary(employee_data)
 
Q.2 Write a Python program to build an SVM model to Cancer dataset. The dataset is available in the scikit-learn library. Check the accuracyof model with precision and recall.
Ans:-import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report
cancer = datasets.load_breast_cancer()
data = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)
data['target'] = cancer.target
X = data.drop('target', axis=1)
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))
 
                                                        SLIP 10
Q.1 Write a R program to find the maximum and the minimum value of a givenvector [10 Marks].
Ans:- my_vector <- c(23, 45, 12, 67, 34, 89, 2, 56, 10)
max_value <- max(my_vector)
min_value <- min(my_vector)
cat("The maximum value in the vector is:", max_value, "\n")
cat("The minimum value in the vector is:", min_value, "\n")
 
Q.2 Write a Python Programme to read the dataset (“Iris.csv”). dataset download from (https://archive.ics.uci.edu/ml/datasets/iris) and apply Apriori algorithm.
Ans:- !pip install apyori  (For Jupyter Notebook)
     	 pip install apyori  (For Terminal)
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from apyori import apriori
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
iris = pd.read_csv(url, header=None, names=columns)
iris['sepal_length'] = pd.cut(iris['sepal_length'], bins=3, labels=['low', 'medium', 'high'])
iris['sepal_width'] = pd.cut(iris['sepal_width'], bins=3, labels=['low', 'medium', 'high'])
iris['petal_length'] = pd.cut(iris['petal_length'], bins=3, labels=['low', 'medium', 'high'])
iris['petal_width'] = pd.cut(iris['petal_width'], bins=3, labels=['low', 'medium', 'high'])
transactions = []
for i in range(0, len(iris)):
    transactions.append([str(iris.iloc[i, j]) for j in range(iris.shape[1])])
rules = apriori(transactions, min_support=0.2, min_confidence=0.6, min_lift=1.0, min_length=2)
results = list(rules)
for rule in results:
    print(f"Rule: {rule.items}")
    print(f"Support: {rule.support}")
    for ordered_stat in rule.ordered_statistics:
        print(f"Confidence: {ordered_stat.confidence}")
        print(f"Lift: {ordered_stat.lift}")
    print("="*40)
       	                                     	SLIP 11
Q.1 Write a R program to find all elements of a given list that are not inanother given list. = list("x", "y", "z") = list("X", "Y", "Z", "x", "y", "z")
Ans:- list1 <- list("x", "y", "z")
list2 <- list("X", "Y", "Z", "x", "y", "z")
vector1 <- unlist(list1)
vector2 <- unlist(list2)
not_in_list2 <- vector1[!vector1 %in% vector2]
cat("Elements in list1 that are not in list2:", not_in_list2, "\n")
 
Q.2 Write a python program to implement hierarchical clustering algorithm.(Download Wholesale customers data dataset from github.com).
Ans:- import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
url = 'wholesale-customer.csv'
df = pd.read_csv(url)
data = df.drop(['Channel', 'Region'], axis=1)
Z = linkage(data, method='ward')
plt.figure(figsize=(12, 8))
dendrogram(Z, truncate_mode='level', p=3)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Customers')
plt.ylabel('Distance')
plt.grid()
plt.show()
n_clusters = 3
clusters = fcluster(Z, n_clusters, criterion='maxclust')
df['Cluster'] = clusters
plt.figure(figsize=(10, 6))
plt.scatter(df['Fresh'], df['Milk'], c=df['Cluster'], cmap='viridis', s=100)
plt.title('Hierarchical Clustering of Wholesale Customers')
plt.xlabel('Fresh Products')
plt.ylabel('Milk Products')
plt.grid()
plt.colorbar(label='Cluster')
plt.show()
                                               	SLIP 12
Q.1 Write a R program to create a Dataframes which contain details of 5employees and display the details. Employee contain (empno,empname,gender,age,designation)
Ans:- employee_data <- data.frame(
  empno = c(101, 102, 103, 104, 105),  # Employee numbers
  empname = c("John", "Alice", "Bob", "Sophia", "David"),  # Employee names
  gender = c("Male", "Female", "Male", "Female", "Male"),  # Gender
  age = c(28, 34, 25, 30, 45),  # Age
  designation = c("Manager", "Developer", "Analyst", "Designer", "Consultant") 
)
print(employee_data)
 
Q.2  Write a python program to implement multiple Linear Regression model for a car dataset. Dataset can be downloaded from: https://www.w3schools.com/python/python_ml_multiple_regression.asp
Ans:- import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
url = "https://www.w3schools.com/python/dataset/cars.csv"
df = pd.read_csv(url)
print("Dataset Preview:")
print(df.head())
X = df[['Weight', 'Volume']]  # Features: Weight and Volume
y = df['CO2']  # Target: CO2 emissions
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("\nModel Coefficients:")
print("Intercept:", model.intercept_)
print("Coefficients:", model.coef_)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("\nModel Performance:")
print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)
comparison = pd.DataFrame({'Actual CO2': y_test, 'Predicted CO2': y_pred})
print("\nComparison of Actual vs Predicted CO2:")
print(comparison)
 
                                                        	SLIP 13
Q.1 Draw a pie chart using R programming for the following data distribution: Digits on Dice 1 2 3 4 5 6 Frequency of getting each number 7 2.
Ans:- Dice<-c(1,2,3,4,5,6)
frequency<-c(7,2,6,3,4,8)
colors<-c("Orange","Blue","Green","Yellow","Pink","Brown")
pie(frequency,label=Dice,main="Data Dirtribution",col=colors)
legend("bottomright",as.character(Dice),fill=colors) 

Q.2 Write a Python program to read “StudentsPerformance.csv” file. Solve following: - To display the shape of dataset. - To display the top rows of the dataset with their columns .Note: Download dataset from following link : (https://www.kaggle.com/spscientist/students-performance-inexams? select=StudentsPerformance.csv)
Ans:- import pandas as pd
data = pd.read_csv("StudentsPerformance.csv")
print("Shape of the dataset (rows, columns):", data.shape)
print("\nTop 5 rows of the dataset:")
print(data.head())
random_rows = data.sample(n=5)
print("\nRandomly selected rows from the dataset:")
print(random_rows)
print("\nNumber of columns:", len(data.columns))
print("Names of columns:", list(data.columns))
 
SLIP 14
Q.1 Write a script in R to create a list of employees (name) and perform the following: a. Display names of employees in the list. b. Add an employee at the end of the list c. Remove the third element of the list.
Ans:- employee_list <- list("John", "Alice", "Bob", "Sophia", "David")
cat("Employee List:\n")
print(employee_list)
new_employee <- "Michael"
employee_list <- append(employee_list, new_employee)
cat("\nEmployee List after adding an employee:\n")
print(employee_list)
employee_list <- employee_list[-3]
cat("\nEmployee List after removing the third element:\n")
print(employee_list)
 
Q.2 Write a Python Programme to apply Apriori algorithm on Groceries dataset. Dataset can be downloaded from (https://github.com/amankharwal/Websitedata/blob/master/Groceries _dataset.csv). Also display support and confidence for each rule.
Ans:- import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules
url="https://raw.githubusercontent.com/amankharwal/Websitedata/master/Groceries_dataset.csv"
df = pd.read_csv(url)
print("Dataset Preview:")
print(df.head())
basket = df.groupby(['Member_number', 'itemDescription']).size().unstack(fill_value=0)
basket = basket.applymap(lambda x: 1 if x > 0 else 0)
frequent_itemsets = apriori(basket, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.2)
print("\nFrequent Itemsets:")
print(frequent_itemsets)
print("\nAssociation Rules with Support and Confidence:")
print(rules[['antecedents', 'consequents', 'support', 'confidence']])
 
SLIP 15
Q.1 Write a R program to add, multiply and divide two vectors of integer type.(vector length should be minimum 4).
Ans:-vector1 <- c(10, 20, 30, 40)
vector2 <- c(2, 4, 6, 8)
addition_result <- vector1 + vector2
multiplication_result <- vector1 * vector2
division_result <- vector1 / vector2
cat("Vector 1:", vector1, "\n")
cat("Vector 2:", vector2, "\n\n")
cat("Addition Result:", addition_result, "\n")
cat("Multiplication Result:", multiplication_result, "\n")
cat("Division Result:", division_result, "\n")
 
Q.2 . Write a Python program build Decision Tree Classifier for shows .csv from pandas and predict class label for show starring a 40 years old American comedian, with 10 years of experience, and a comedy ranking of 7? Create a csv file as shown in https://www.w3schools.com/python/python_ml_decision_tree.asp
Ans:- import pandas as pd
from sklearn.tree import DecisionTreeClassifier
data = pd.read_csv('shows.csv')
print("Dataset Preview:")
print(data.head())
data['Nationality'] = data['Nationality'].map({'UK': 0, 'USA': 1, 'N': 2})
data['Go'] = data['Go'].map({'No': 0, 'Yes': 1})
X = data[['Age', 'Experience', 'Rank', 'Nationality']]  # Features
y = data['Go']  # Target
model = DecisionTreeClassifier()
model.fit(X, y)
new_show = [[40, 10, 7, 1]]  # [Age, Experience, Rank, Nationality (1 for USA)]
prediction = model.predict(new_show)
if prediction == 1:
    print("Prediction: The show will be successful (Yes).")
else:
    print("Prediction: The show will not be successful (No).")
        
                                                          SLIP 16
Q.1 . Write a R program to create a simple bar plot of given data Year Export Import 2001 26 35 2002 32 40 2003 35 50.
Ans:- Year <- c(2001, 2002, 2003)
Export <- c(26, 32, 35)
Import <- c(35, 40, 50)
data <- data.frame(Year, Export, Import)
barplot(
  height = rbind(data$Export, data$Import),
  beside = TRUE,                           
  names.arg = data$Year,
  col = c("blue", "red"),               	
  legend.text = c("Export", "Import"),  	
  main = "Export and Import for Different Years",
  xlab = "Year",                        	
  ylab = "Value"               
)
 
Q.2 Write a Python program build Decision Tree Classifier using Scikit-learnpackage for diabetes data set (download database from https://www.kaggle.com/uciml/pima-indiansdiabetes-database)
Ans:- import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn import tree
url = 'https://raw.githubusercontent.com/uciml/pima-indians-diabetes-database/master/diabetes.csv'
data = pd.read_csv(url)
print("Dataset Preview:")
print(data.head())
X = data.drop(columns='Outcome')  # Features: All columns except 'Outcome'
y = data['Outcome']  # Target: The 'Outcome' column
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
tree.plot_tree(model, feature_names=X.columns, class_names=["No Diabetes", "Diabetes"], filled=True)
plt.show()
 
             	                                         	SLIP 17
Q.1 Write a R program to get the first 20 Fibonacci numbers.
Ans:- fibo<-function(num){
  x<-0
  y<-1
  print("Fibonnaci Series  Are :")
  print(x)
  print(y)
  for(i in (3:num)){
   z<-x+y
   print(z)
   x<-y
   y<-z
 }
}
fibo(20)
Q.2 Write a python programme to implement multiple linear regression modelfor stock market data frame as follows: Stock_Market = {'Year': [2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2016,2 016,20,16,2016,2016,2016,2016,2016,2016,2016,2016,2016], 'Month': [12, 11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1], 'Interest_Rate': [2.75,2.5,2.5,2.5,2.5,2.5,2.5,2.25,2.25,2.25,2,2,2,1.75,1.75,1.75,1.75,1.75,1 .75,1.75,1.75,1.75,1.75,1.75], 'Unemployment_Rate': [5.3,5.3,5.3,5.3,5.4,5.6,5.5,5.5,5.5,5.6,5.7,5.9,6,5.9,5.8,6.1,6.2,6.1,6.1,6.1,5 .9,6.2,6.2,6.1], 'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075,1047, 965,943,958,971,949,884,866,876,822,704,719] } And draw a graph of stock market price verses interest rate.
Ans:- import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
Stock_Market = {
    'Year': [2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017,
         	2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016],
    'Month': [12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1,
          	12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],
    'Interest_Rate': [2.75, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.25, 2.25, 2.25, 2, 2,
                  	1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75],
    'Unemployment_Rate': [5.3, 5.3, 5.3, 5.3, 5.4, 5.6, 5.5, 5.5, 5.5, 5.6, 5.7, 5.9,
                      	6, 5.9, 5.8, 6.1, 6.2, 6.1, 6.1, 6.1, 5.9, 6.2, 6.2, 6.1],
    'Stock_Index_Price': [1464, 1394, 1357, 1293, 1256, 1254, 1234, 1195, 1159, 1167,
                      	1130, 1075, 1047, 965, 943, 958, 971, 949, 884, 866, 876, 822, 704, 719]
}
df = pd.DataFrame(Stock_Market)
X = df[['Year', 'Month', 'Interest_Rate', 'Unemployment_Rate']] 
y = df['Stock_Index_Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
plt.figure(figsize=(10, 6))
sns.scatterplot(x=df['Interest_Rate'], y=df['Stock_Index_Price'], color='blue', label='Actual Data')
plt.title('Stock Index Price vs Interest Rate')
plt.xlabel('Interest Rate')
plt.ylabel('Stock Index Price')
plt.grid(True)
plt.legend()
plt.show()
 
                                                        	SLIP 18
Q.1 Write a R program to find the maximum and the minimum value of a given vector.
Ans:-my_vector <- c(12, 45, 23, 67, 34, 89, 21, 78)
max_value <- max(my_vector)
cat("The maximum value in the vector is:", max_value, "\n")
min_value <- min(my_vector)
cat("The minimum value in the vector is:", min_value, "\n")
 
Q.2 Consider the following observations/data. And apply simple linear regression and find out estimated coefficients b1 and b1 Also analyse the  performance of the model (Use sklearn package) x = np.  array([1,2,3,4,5,6,7,8]) y = np.array([7,14,15,18,19,21,26,23])
Ans:- import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([7, 14, 15, 18, 19, 21, 26, 23])
x = x.reshape(-1, 1)
model = LinearRegression()
model.fit(x, y)
b0 = model.intercept_  # This is b0 (intercept)
b1 = model.coef_[0]	# This is b1 (slope)
print(f"Estimated coefficients:")
print(f"b0 (intercept): {b0}")
print(f"b1 (slope): {b1}")
y_pred = model.predict(x)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)
print(f"\nModel Performance:")
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
plt.figure(figsize=(10, 6))
plt.scatter(x, y, color='blue', label='Actual data')
plt.plot(x, y_pred, color='red', label='Fitted line')
plt.title('Simple Linear Regression')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid()
plt.show()
 
                                       	              	SLIP 19
Q.1 . Write aR program to create a Dataframes which contain details of 5 Studentsand display the details.
Ans:- students <- data.frame(
  Student_ID = c(1, 2, 3, 4, 5),
  Name = c("Alice", "Bob", "Charlie", "David", "Eva"),
  Age = c(20, 21, 22, 23, 20),
  Major = c("Mathematics", "Physics", "Chemistry", "Biology", "Computer Science"),
  Grade = c("A", "B", "A", "C", "B")
)
print("Details of Students:")
print(students)
 
Q.2 Write a python program to implement multiple Linear Regression modelfor a car dataset. Dataset can be downloaded from: https://www.w3schools.com/python/python_ml_multiple_regression.asp
Ans:- import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
url = "https://www.w3schools.com/python/datasets/cars.csv"
data = pd.read_csv(url)
print("Dataset Head:")
print(data.head())
# We will use 'Horsepower', 'Weight', and 'Year' as features
# and 'MPG' (Miles Per Gallon) as the target variable
X = data[['Horsepower', 'Weight', 'Year']]
y = data['MPG']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("\nModel Performance:")
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
print("\nEstimated coefficients:")
print(f"Intercept (b0): {model.intercept_}")
print(f"Coefficients (b1, b2, b3): {model.coef_}")
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2, label='Perfect Prediction')
plt.xlabel('Actual MPG')
plt.ylabel('Predicted MPG')
plt.title('Actual vs Predicted MPG')
plt.legend()
plt.grid()
plt.show()
                      	                           	SLIP 20
Q.1 Write a R program to create a data frame from four given vectors.
Ans:- student_ids <- c(1, 2, 3, 4, 5)
student_names <- c("Alice", "Bob", "Charlie", "David", "Eva")
student_ages <- c(20, 21, 22, 23, 20)
student_grades <- c("A", "B", "A", "C", "B")
students <- data.frame(
  Student_ID = student_ids,
  Name = student_names,
  Age = student_ages,
  Grade = student_grades
)
print("Student Details DataFrame:")
print(students)
 
 
Q2) Write a python program to implement hierarchical Agglomerativeclustering
algorithm. (Download Customer.csv dataset from github.com).
ANS:
url---> https://gist.github.com/akuks/2e9b08cebef0181b583a1dff4a97f8a1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
# Load the dataset directly from the URL
url = 'Customer.csv' # Replace with the actual URL
df = pd.read_csv(url)
# Display the first few rows of the dataset
print(df.head())
# Check for missing values
print(df.isnull().sum())
# Drop rows with missing values (if any)
df.dropna(inplace=True)
# Convert DOB to age - specify the date format
df['DOB'] = pd.to_datetime(df['DOB'], format='%d/%m/%y %H:%M',
dayfirst=True) # Adjust the format for day first
df['Age'] = (pd.Timestamp.now() - df['DOB']).dt.days // 365 # Calculate age in
years
# Select relevant features for clustering
features = df[['Age', 'Gender']]
# One-hot encode the categorical 'Gender' feature
features_encoded = pd.get_dummies(features, columns=['Gender'],
drop_first=True)
# Check the resulting DataFrame after encoding
print("Encoded Features:\n", features_encoded.head())
print("Columns after Encoding:", features_encoded.columns.tolist())
# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_encoded)
# Perform Hierarchical Agglomerative Clustering
model = AgglomerativeClustering(n_clusters=4) # Adjust the number of clusters
as needed
labels = model.fit_predict(features_scaled)
# Add cluster labels to the original dataframe
df['Cluster'] = labels
# Display the first few rows with cluster labels
print(df.head())
# Check the unique labels
print("Unique Cluster Labels:", df['Cluster'].unique())
# Optional: Plotting the clusters (use only Age and one gender column for
visualization)
gender_column = features_encoded.columns[1] # Assuming the first column is
'Age'
plt.figure(figsize=(10, 6))
plt.scatter(df['Age'], features_encoded[gender_column], c=labels, cmap='viridis',
marker='o', edgecolor='k')
plt.title('Hierarchical Agglomerative Clustering of Customers')
plt.xlabel('Age')
plt.ylabel(gender_column) # Update the y-label to match the gender column
plt.colorbar(label='Cluster')
plt.grid(True) # Add grid for better readability
plt.show()
# Optional: Dendrogram for visualizing hierarchical clustering
plt.figure(figsize=(10, 6))
from scipy.cluster.hierarchy import dendrogram, linkage
linked = linkage(features_scaled, 'ward')
dendrogram(linked,
 orientation='top',
 distance_sort='descending',
 show_leaf_counts=True)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.grid(True) # Add grid for better readability
plt.show(

